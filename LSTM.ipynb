{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pairs = [\n",
    "    (\"I love you\", \"मैं तुमसे प्यार करता हूँ\"),\n",
    "    (\"How are you\", \"तुम कैसे हो\"),\n",
    "    (\"Good morning\", \"सुप्रभात\"),\n",
    "    (\"Thank you\", \"धन्यवाद\"),\n",
    "    (\"What is your name\", \"तुम्हारा नाम क्या है\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<eos>': 2, '<pad>': 0, '<sos>': 1, 'i': 7, 'you': 3, 'how': 6, 'are': 4, 'good': 5, 'is': 8, 'love': 9, 'what': 13, 'morning': 10, 'name': 11, 'thank': 12, 'your': 14}\n",
      "{'करता': 3, '<eos>': 2, '<pad>': 0, 'हो': 16, 'मैं': 12, '<sos>': 1, 'कैसे': 4, 'तुमसे': 7, 'तुम': 6, 'क्या': 5, 'तुम्हारा': 8, 'धन्यवाद': 9, 'नाम': 10, 'प्यार': 11, 'सुप्रभात': 13, 'हूँ': 14, 'है': 15}\n",
      "5\n",
      "[(tensor([1, 7, 9, 3, 2]), tensor([ 1, 12,  7, 11,  3, 14,  2])), (tensor([1, 6, 4, 3, 2]), tensor([ 1,  6,  4, 16,  2])), (tensor([ 1,  5, 10,  2]), tensor([ 1, 13,  2])), (tensor([ 1, 12,  3,  2]), tensor([1, 9, 2])), (tensor([ 1, 13,  8, 14, 11,  2]), tensor([ 1,  8, 10,  5, 15,  2]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Python projects\\RNN2\\environm\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5116\\234665764.py\", line 27, in <module>\n",
      "    dataset = [(sentence_to_tensor(en, english_vocab, \"en\"),\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5116\\234665764.py\", line 27, in <listcomp>\n",
      "    dataset = [(sentence_to_tensor(en, english_vocab, \"en\"),\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5116\\234665764.py\", line 24, in sentence_to_tensor\n",
      "    return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n",
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_5116\\234665764.py:24: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer function\n",
    "def tokenize(sentence, lang=\"en\"):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(sentences, lang):\n",
    "    tokenized_sentences = [tokenize(sentence, lang) for sentence, _ in sentences] if lang == \"en\" else \\\n",
    "                          [tokenize(sentence, lang) for _, sentence in sentences]\n",
    "    vocab = build_vocab_from_iterator(tokenized_sentences, specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    return vocab\n",
    "\n",
    "# Create vocabularies\n",
    "english_vocab = build_vocab(data_pairs, lang=\"en\")\n",
    "hindi_vocab = build_vocab(data_pairs, lang=\"hi\")\n",
    "\n",
    "print(english_vocab.get_stoi())\n",
    "print(hindi_vocab.get_stoi())\n",
    "\n",
    "\n",
    "# Convert sentence to tensor\n",
    "def sentence_to_tensor(sentence, vocab, lang):\n",
    "    tokens = [\"<sos>\"] + tokenize(sentence, lang) + [\"<eos>\"]\n",
    "    return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n",
    "\n",
    "# Prepare dataset\n",
    "dataset = [(sentence_to_tensor(en, english_vocab, \"en\"), \n",
    "            sentence_to_tensor(hi, hindi_vocab, \"hi\")) for en, hi in data_pairs]\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output: tensor([[-0.0728,  0.0599,  0.1115, -0.0900,  0.0810,  0.0574, -0.0314, -0.0424,\n",
      "         -0.0938,  0.0819,  0.0685, -0.1414,  0.0866,  0.0762,  0.0793, -0.0443],\n",
      "        [ 0.0667, -0.0740,  0.0117, -0.0202, -0.0781, -0.0460,  0.1325,  0.1167,\n",
      "          0.0564, -0.0996,  0.0701, -0.0306, -0.0058, -0.0166,  0.2626,  0.0690],\n",
      "        [-0.0056, -0.0327,  0.0977, -0.0311, -0.0192,  0.0157,  0.1248, -0.0133,\n",
      "         -0.0413,  0.0162,  0.1353, -0.1352,  0.1044,  0.0410,  0.1984, -0.0113],\n",
      "        [-0.0124, -0.1064,  0.1757, -0.0285, -0.0355, -0.0658, -0.0398,  0.0899,\n",
      "         -0.1370, -0.0074,  0.2333, -0.0265, -0.1212, -0.0622,  0.1096,  0.1029],\n",
      "        [-0.0773, -0.1206,  0.3438,  0.0586, -0.0396, -0.0912, -0.0255,  0.0357,\n",
      "         -0.1070,  0.0714,  0.2950,  0.0842, -0.1984, -0.1016,  0.1229, -0.0059]],\n",
      "       grad_fn=<SqueezeBackward1>) encoder hidden: (tensor([[-0.0773, -0.1206,  0.3438,  0.0586, -0.0396, -0.0912, -0.0255,  0.0357,\n",
      "         -0.1070,  0.0714,  0.2950,  0.0842, -0.1984, -0.1016,  0.1229, -0.0059]],\n",
      "       grad_fn=<SqueezeBackward1>), tensor([[-0.1610, -0.2765,  0.6815,  0.1971, -0.1439, -0.3914, -0.0711,  0.0907,\n",
      "         -0.2694,  0.1955,  0.6039,  0.1954, -0.3776, -0.2413,  0.1912, -0.0144]],\n",
      "       grad_fn=<SqueezeBackward1>))\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(english_vocab) # 15\n",
    "embed_dim =8\n",
    "hidden_dim = 16\n",
    "\n",
    "encoder = Encoder(input_dim, embed_dim, hidden_dim)\n",
    "sentence = \"I love you\"\n",
    "sentence_tensor = sentence_to_tensor(sentence, english_vocab, \"en\")\n",
    "encoder_output, encoder_hidden = encoder(sentence_tensor)\n",
    "print(\"encoder output:\", encoder_output, \"encoder hidden:\", encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(0)  # Add time dimension\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        prediction = self.fc(output.squeeze(0))\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        outputs = torch.zeros(trg.size(0), trg.size(1), len(hindi_vocab))  # Preallocate space\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[0, :]  # Start token\n",
    "        for t in range(1, trg.size(0)):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[t] = output\n",
    "            input = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else output.argmax(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 14.5820\n",
      "Epoch 2/10, Loss: 11.0276\n",
      "Epoch 3/10, Loss: 8.3561\n",
      "Epoch 4/10, Loss: 6.1859\n",
      "Epoch 5/10, Loss: 3.9467\n",
      "Epoch 6/10, Loss: 1.9215\n",
      "Epoch 7/10, Loss: 0.7741\n",
      "Epoch 8/10, Loss: 0.3552\n",
      "Epoch 9/10, Loss: 0.1773\n",
      "Epoch 10/10, Loss: 0.1035\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_DIM = len(english_vocab)\n",
    "OUTPUT_DIM = len(hindi_vocab)\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "encoder = Encoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=hindi_vocab[\"<pad>\"])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    for src, trg in dataset:\n",
    "        src, trg = src.unsqueeze(1), trg.unsqueeze(1)  # Add batch dimension\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मैं तुमसे प्यार करता हूँ\n"
     ]
    }
   ],
   "source": [
    "def translate_sentence(sentence, model, english_vocab, hindi_vocab):\n",
    "    model.eval()\n",
    "    src_tensor = sentence_to_tensor(sentence, english_vocab, \"en\").unsqueeze(1)\n",
    "    encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    trg_tokens = [\"<sos>\"]\n",
    "    input_token = torch.tensor([hindi_vocab[\"<sos>\"]])\n",
    "    while input_token.item() != hindi_vocab[\"<eos>\"]:\n",
    "        output, hidden = model.decoder(input_token, hidden)\n",
    "        input_token = output.argmax(1)\n",
    "        trg_tokens.append(hindi_vocab.lookup_token(input_token.item()))\n",
    "    return \" \".join(trg_tokens[1:-1])  # Exclude <sos> and <eos>\n",
    "\n",
    "# Test translation\n",
    "print(translate_sentence(\"I love you\", model, english_vocab, hindi_vocab))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
